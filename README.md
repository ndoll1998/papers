# papers

## machine learning

 - [x] [“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors](https://aclanthology.org/2023.findings-acl.426/)
 - [ ] [Ml-knn: A Lazy Learning Approach to Multi-Label Learning](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/pr07.pdf)

### optimization

 - [ ] [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/pdf/2203.03466.pdf)
 - [ ] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)

### transformers / LLMs

 - [x] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
 - [x] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
 - [ ] [GPT: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
 - [ ] [GPT2: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
 - [ ] [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://scontent-fra3-1.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=qhK-ahCbkBMAX-aOz9J&_nc_ht=scontent-fra3-1.xx&oh=00_AfDLDaEewnAZWDbP-GGxfz1KwBfX25hMX_2MD2KcOLZmow&oe=64BE66FF)
 - [ ] [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)

#### surveys
  
 - [ ] [A survey of transformers](https://www.sciencedirect.com/science/article/pii/S2666651022000146)
 - [ ] [Efficient Transformers: A Survey](https://dl.acm.org/doi/pdf/10.1145/3530811)
 - [ ] [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
 - [ ] [Augmented Language Models: a Survey](https://arxiv.org/pdf/2302.07842.pdf)

#### scaling

 - [ ] [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
 - [ ] [Scaling Laws for BERT in Low-Resource Settings](https://aclanthology.org/2023.findings-acl.492.pdf)
 - [ ] [Examining Scaling and Transfer of Language Model Architectures for Machine Translation](https://arxiv.org/abs/2202.00528)

#### embeddings

 - [ ] [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409v2)
 - [ ] [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864v4)
 - [ ] [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)

#### attention

 - [ ] [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf)
 - [ ] [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691.pdf)
 - [ ] [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/pdf/2302.10866.pdf)
 - [ ] [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150)
 - [ ] [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)

#### fine-tuning

 - [ ] [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/pdf/2305.17333.pdf)
 - [ ] [Full Parameter Fine-Tuning for Large Language Models with Limited Resources](https://arxiv.org/pdf/2306.09782.pdf)
 - [ ] [Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models](https://arxiv.org/abs/2203.06904)
 - [ ] [Large Language Models Can Self-Improve](https://arxiv.org/abs/2210.11610)
 - [ ] [How Many Data Points is a Prompt Worth?](https://arxiv.org/abs/2103.08493)

#### adapters

 - [ ] [Robust Transfer Learning with Pretrained Language Models through Adapters](https://arxiv.org/pdf/2108.02340.pdf)
 - [ ] [P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts](https://arxiv.org/pdf/2110.07280.pdf)
 - [ ] [ADAMIX: Mixture-of-Adapter for Parameter-Efficient tuning of Large Language Models](https://www.microsoft.com/en-us/research/uploads/prod/2022/05/Mixture_of_Adapters-628fa6a57efd3.pdf)
 - [ ] [AdapterDrop: On the Efficiency of Adapters in Transformers](https://aclanthology.org/2021.emnlp-main.626.pdf)
 - [ ] [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
 - [ ] [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
 - [ ] [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)
 - [ ] [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
 - [ ] [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
 - [ ] [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/)

#### frameworks

 - [ ] [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
 - [ ] [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf)
